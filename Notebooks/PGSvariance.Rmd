---
title: "generateGeno"
output: html_document
date: "2025-06-12"
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ggplot2)
```

# Uncertainty in PGS estimates

Generate allele 1 frequencies from a uniform distribution on (0,1).
We assume allele frequencies to follow HWE.

```{r}
M = 10000 # of SNPs, test with different ones
N = 1000 # of individuals, fixed at first
ps = runif(M, 0, 1) #  allele 1 frequencies for M SNPs

h2 = 0.5 # assumed heritability, TODO: vary heritability 
```

#### Simplified effect sizes

At first, the true effect sizes are assumed to come from normal distribution $\beta \sim N(0, 1)$.
TODO: correct distribution parameters

Simulate true effect sizes and effect size estimates with noise proportional to sample size.

```{r}
betas.s = rnorm(M, 0, 1) # draw true effect sizes from standard normal distribution
beta.s.est = betas.s + 1/N * rnorm(M, 0, 1) # add noise proportional to sample size
```

#### Effect sizes taking into account negative selection

We account for negative selection by assuming that the magnitude of effects $\beta^2$ is proportional to $(2p(1-p))^S$, where $S=-1$.
This is done by drawing effect estimates from a multinomial normal distribution with mean 0 and variance of $(2p(1-p))^S$.
Here we assume each SNP contributes equally to the total variance, with the variances summing up to $h^2$.
Draw true effects from a normal distribution, which sd is proportional to heritability.
The effects are estimated as in GWAS, by adding a random error term to the true estimate: $\hat{\beta} = \beta + \epsilon$, where $\epsilon \sim N(0,1/N)$.

```{r}
S = -1 # relative contributions of common vs rare variants
var.neg = h2/((2*ps*(1-ps))^S*M) # variance proportional to negative selection coefficient, scaled to match heritability h2 and M SNPs
betas.neg = rnorm(M, mean = 0, sd = sqrt(var.neg)) # draw betas with variance var.neg

beta.neg.est = betas.neg + 1/N * rnorm(M, 0, 1) # add noise proportional to sample size
summary(beta.neg.est)
```

**TODO**: plot 1/N against PGS variance

Generate genotypes for N = {1000, 10000, 100000, 500000} individuals at 1000 loci assuming allele frequencies follow HWE.

```{r}
genotypes = matrix(nrow = N, ncol = M)
for (i in 1:M) {
  genotypes[, i] = rbinom(N, size = 2, prob = ps[i])
}
# TODO: vectorize for performance

```

#### Individual PGS estimates

For each individual, we compute the PGS estimate as a weighted sum of their genotypes and corresponding effect estimates.
**Note!** Each individual has their own genotypes, but the same effect estimates.
We also compute the uncertainty in the effect estimate as a variance of the linear combination, assuming the random errors are independent.

```{r}
pgs = genotypes %*% beta.neg.est # compute PGS accounting for negative selection
pgs.var = 1/N * rowSums(genotypes^2) # variance in individual PGS estimates due to noise in the effect size estimates
```

#### Individual PGS estimates by sampling

TODO: how to get the variance in the beta estimates, which formula to use for PGS variance TODO: Try also: generate 1000 samples of the beta estimates, and for each sample compute the PGS for each individual, and compute the variance of these estimates.

Then, adjust the simulation by assuming negative selection: the magnitude of effects $\beta^2$ is proportional to $(2p(1-p))^S$, where $S=-1$.
(maybe sample betas from a standard normal distribution and the scale with the formula)

```{r}
source("PGSvariance.R")
pgs.unc = PGS.uncertainty(500000, 1000, 0.5, -1)
summary(pgs.unc$pgs)
summary(pgs.unc$variance)
```

```{r}
source("PGSvariance.R")
pgs.sample.unc = PGS.sample.uncertainty(1000, 10000, 0.5, -1, 1000)
summary(pgs.sample.unc$pgs)
summary(pgs.sample.unc$variance)
#ordered.estimates(pgs.sample.unc$pgs, pgs.sample.unc$variance, 0.99, 0.95)

```

#### Sampled PGS variance with different N
We study the variance in the PGS estimates caused by the uncertainty in the effect estimates by calculating the PGS variance analytically as a variance of the linear combination. We notice that, as expected, the variance in the individual PGS estimated decreases with increasing sample size.

```{r}
source("PGSvariance.R")
ns = c(100, 1000, 10000, 50000)
M = 10000 # of SNPs
h2 = 0.5 # heritability
S = -1
nsamp = 1000
pgs.vars = vector("list", length(ns))

for (i in 1:length(ns)){
  pgs.vars[[i]] = PGS.uncertainty(ns[i], M, h2, S)$variance
}
```

```{r}

all.pgs.var = unlist(pgs.vars)

# a matching vector of sample sizes for each variance value
sample.size = rep(ns, times = ns)
# a data frame for plotting
df = data.frame(N = sample.size, Variance = all.pgs.var)

ggplot(df, aes(x = N, y = Variance)) +
  geom_point(alpha = 0.2, color = "steelblue") +
  scale_x_log10() +
  scale_y_log10() +
  labs(title = "PGS Variance Across Sample Sizes",
       x = "Sample Size (log10 scale)",
       y = "Variance of PGS") 
```

### Sample size vs. individual PGS variance
We further simulate the variance in the individual PGS estimates as a function of GWAS training sample size N and heritability h2. We conclude that the variance decreases with both increasing sampels size N and increasing heritability. This makes sense as when larger proportion of the variation in the phenotype is explained by the genetic effects, we can estimate the genetic effect more precisely. 
```{r}
rho = 0.95 # confidence level
S = -1 # selection coefficient
```

M = 1000 and N = 1000, 10 000, 100 000, 250 000, 500 000 - h2 = 0.1, 0.25, 0.5, 0.8

```{r}
source("PGSvariance.R")
M = 1000 # of SNPs
h2s = c(0.1, 0.25, 0.5, 0.8) # heritabilities
Ns = c(1000, 10000, 100000, 250000, 500000) # of individuals
t = 0.9 # 
n = 10000 
certain.proportions.s1 = matrix(nrow = length(Ns), ncol = 2*length(h2s))
df.var.list = list() # list to store data frames
for (j in 1:length(h2s)){
  pgs = vector("list", length(Ns))
  pgs.vars = vector("list", length(Ns))
  ordered = matrix(nrow = length(Ns), ncol = 2) # matrix for storing proportions of certain PGS estimates
  
  for (i in 1:length(Ns)){
    pgs.unc = PGS.uncertainty(Ns[i], M, h2s[j], S)
    pgs[[i]] = pgs.unc$pgs # PGSs for N = Ns[i]
    pgs.vars[[i]] = pgs.unc$variance # variances of PGSs for N = Ns[i]
    ord = ordered.estimates(pgs.unc$pgs, pgs.unc$variance, t, rho) # collect certain proportions
    ordered[i, ] = c(ord$prop.certain.below, ord$prop.certain.above)
  }
  ind = 2*j - 1
  certain.proportions.s1[, (ind):(ind+1)] = ordered # proportions of certain-below-t and certain-above-t
  
  all.var = unlist(pgs.vars) # flatten
  N.rep = rep(Ns, times = sapply(pgs.vars, length))  # match each variance to its sample size
  df.var.list[[j]] = data.frame( # data frame of variances for heritability h2s[j]
    N = N.rep,
    Variance = all.var,
    h2 = h2s[j]
  )
 }
```

We demonstrate how the proportion of individuals certain-above/below the PGS risk threshold increases with sample size and heritability.
```{r}
#Label the rows by sample size and columns by heritability + position
rownames(certain.proportions.s1) = paste0("N=", Ns)
colnames(certain.proportions.s1) = as.vector(t(outer(
  h2s, c("Below", "Above"),
  FUN = function(h, dir) paste0("h2=", h, "_", dir)
)))
certain.proportions.s1 

print(round(certain.proportions.s1, 3)) # as table in R

library(knitr) # clean table
kable(certain.proportions.s1, digits = 3, caption = "Proportion of Certain PGS Estimates (Below/Above Threshold)")

kable(certain.proportions.s1[, 3:6]) # for h = 0.25 and 0.5

```

```{r}

# Create a clean long-format data frame from certain.proportions.s1
df.prop = data.frame(
  SampleSize = rep(Ns, times = length(h2s) * 2),
  h2 = rep(rep(h2s, each = 2), each = length(Ns)),
  Direction = rep(rep(c("Below", "Above"), times = length(h2s)), each = length(Ns)),
  Proportion = as.vector(certain.proportions.s1)
)

# Ensure factors for plotting
df.prop$SampleSize = factor(df.prop$SampleSize, levels = Ns)
df.prop$h2 = factor(df.prop$h2, levels = h2s)

# Plot with ggplot2
library(ggplot2)
ggplot(df.prop, aes(x = SampleSize, y = Proportion, fill = Direction)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ h2, labeller = label_bquote(h^2 == .(as.numeric(as.character(h2))))) +
  labs(
    title = "Proportion of Certain PGS Estimates",
    y = "Proportion",
    x = "Sample Size"
  ) +
  theme_minimal()



```
The mean variance decreases with sample size and heritability.
```{r}
df.var = do.call(rbind, df.var.list)
df.var$h2 = factor(df.var$h2)

df.mean.var <- aggregate(Variance ~ N + h2, data = df.var, FUN = mean) # compute mean of variances
colnames(df.mean.var)[3] <- "MeanVariance"
df.mean.var$MeanVariance <- round(df.mean.var$MeanVariance, 3)

library(ggplot2)

ggplot(df.mean.var, aes(x = N, y = MeanVariance, color = h2)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  scale_x_log10() +
  scale_y_log10() +
  labs(
    title = "Mean PGS Variance vs Sample Size",
    x = "Sample Size (log scale)",
    y = "Mean Variance of PGS (log scale)",
    color = "Heritability (hÂ²)"
  ) +
  theme_minimal()
#df.mean.var
df.mean.var$MeanVariance

df.wide <- reshape(df.mean.var, timevar = "h2", idvar = "N", direction = "wide") # print table
colnames(df.wide) <- c("N", paste0("h2 = ", levels(df.var$h2)))

colnames(df.wide) <- sub("Variance\\.", "", colnames(df.wide))
kable(df.wide)

```

The uncertainty of the PGS estimates decreases with sample size and heritability.
The lines corresponding to different heritabilities are overlapping.

M = 10 000 and N = 1000, 10 000, 50 000 - h2 = 0.1, 0.25, 0.5, 0.8

```{r}
M = 10000
h2s = c(0.1, 0.25, 0.5, 0.8) # heritabilities
Ns = c(1000, 10000, 50000) # of individuals
t = 0.9 # 
certain.proportions = matrix(nrow = length(Ns), ncol = 2*length(h2s))
df.var.list = list() # list to store data frames
for (j in 1:length(h2s)){
  pgs = vector("list", length(Ns))
  pgs.vars = vector("list", length(Ns))
  ordered = matrix(nrow = length(Ns), ncol = 2) # matrix for storing proportions of certain PGS estimates
  
  for (i in 1:length(Ns)){
    pgs.unc = PGS.uncertainty(Ns[i], M, h2s[j], S)
    pgs[[i]] = pgs.unc$pgs # PGSs for N = Ns[i]
    pgs.vars[[i]] = pgs.unc$variance # variances of PGSs for N = Ns[i]
    ord = ordered.estimates(pgs.unc$pgs, pgs.unc$variance, t, rho)
    ordered[i, ] = c(ord$prop.certain.below, ord$prop.certain.above)
  }
  ind = 2*j - 1
  certain.proportions[, (ind):(ind+1)] = ordered # proportions of certain-below-t and certain-above-t
  
  all.var = unlist(pgs.vars) # flatten
  N.rep = rep(Ns, times = sapply(pgs.vars, length))  # match each variance to its sample size
  df.var.list[[j]] = data.frame( # data frame of variances for heritability h2s[j]
    N = N.rep,
    Variance = all.var,
    h2 = h2s[j])
}

```

```{r}
#Label the rows by sample size and columns by heritability + position
rownames(certain.proportions) = paste0("N=", Ns)
colnames(certain.proportions) = as.vector(t(outer(
  h2s, c("Below", "Above"),
  FUN = function(h, dir) paste0("h2=", h, "_", dir)
)))
certain.proportions 

print(round(certain.proportions, 3)) # as table in R

library(knitr) # clean table
kable(certain.proportions, digits = 3, caption = "Proportion of Certain PGS Estimates (Below/Above Threshold)")

kable(certain.proportions[, 3:6]) # for h = 0.25 and 0.5
```

```{r}
df.var = do.call(rbind, df.var.list)
df.var$h2 = factor(df.var$h2)

library(dplyr)

df.mean.var = df.var %>%
  group_by(N, h2) %>%
  summarize(MeanVariance = mean(Variance), .groups = "drop")


library(ggplot2)

ggplot(df.mean.var, aes(x = N, y = MeanVariance, color = h2, lty = h2)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  scale_x_log10() +
  scale_y_log10() +
  labs(
    title = "Mean PGS Variance vs Sample Size",
    x = "Sample Size (log scale)",
    y = "Mean Variance of PGS (log scale)",
    color = "Heritability (hÂ²)"
  ) +
  theme_minimal()
df.mean.var
str(df.mean.var)
```

With small sample size such as N = 50 000 or below, the uncertainty in the PGS estimates remains high.

### Number of SNPs vs individual PGS variance

N = 50 000 and M = 500, 1000, 5 000, 10 000 - h2 = 0.1, 0.25, 0.5, 0.8

```{r}
N = 50000 # of individuals
h2s = c(0.1, 0.25, 0.5, 0.8) # heritabilities
Ms = c(500, 1000, 5000, 10000) # of SNPs
t = 0.9 # 
certain.proportions = matrix(nrow = length(Ms), ncol = 2*length(h2s))
for (j in 1:length(h2s)){
  pgs = vector("list", length(Ms))
  pgs.vars = vector("list", length(Ms))
  ordered = matrix(nrow = length(Ms), ncol = 2) # matrix for storing proportions of certain PGS estimates
  
  for (i in 1:length(Ms)){
    pgs.unc = PGS.uncertainty(N, Ms[i], h2s[j], S)
    pgs[[i]] = pgs.unc$pgs # PGSs for M = Ms[i]
    pgs.vars[[i]] = pgs.unc$variance # variances of PGSs for M = Ms[i]
    ord = ordered.estimates(pgs.unc$pgs, pgs.unc$variance, t, rho)
    ordered[i, ] = c(ord$prop.certain.below, ord$prop.certain.above)
  }
  certain.proportions[, (2*j-1):(2*j)] = ordered # proportions of certain-below-t and certain-above-t
  
  all.var = unlist(pgs.vars) # flatten
  N.rep = rep(Ms, times = sapply(pgs.vars, length))  # match each variance to its sample size
  df.var.list[[j]] = data.frame( # data frame of variances for heritability h2s[j]
    N = N.rep,
    Variance = all.var,
    h2 = h2s[j]
  )
  
}

```

```{r}
#Label the rows by sample size and columns by heritability + position
rownames(certain.proportions) = paste0("M=", Ms)
colnames(certain.proportions) = as.vector(t(outer(
  h2s, c("Below", "Above"),
  FUN = function(h, dir) paste0("h2=", h, "_", dir)
)))
certain.proportions 

print(round(certain.proportions, 3)) # as table in R

library(knitr) # clean table
kable(certain.proportions, digits = 3, caption = "Proportion of Certain PGS Estimates (Below/Above Threshold)")

kable(certain.proportions[, 3:6]) # only h = 0.25 and 0.5
```

The proportion of certain above/below-threshold PGSs decreases with number of SNPs, as more SNPs accumulate more variation.
As before, the uncertainty decreases with heritability.

```{r}

df.var <- do.call(rbind, df.var.list) # combine data
df.var$h2 <- factor(df.var$h2)

df.mean.var <- aggregate(Variance ~ N + h2, data = df.var, FUN = mean) # mean and var by M and h2
colnames(df.mean.var)[3] <- "MeanVariance"
df.mean.var$MeanVariance <- round(df.mean.var$MeanVariance, 5)

df.wide <- reshape(df.mean.var, timevar = "h2", idvar = "N", direction = "wide")
colnames(df.wide) <- c("M", paste0("h2 = ", levels(df.var$h2)))
kable(df.wide)

library(ggplot2)

ggplot(df.mean.var, aes(x = N, y = MeanVariance, color = h2)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  scale_x_log10() +
  scale_y_log10() +
  labs(
    title = "Mean PGS Variance vs Sample Size",
    x = "Sample Size (log scale)",
    y = "Mean Variance of PGS (log scale)",
    color = "Heritability (hÂ²)"
  ) +
  theme_minimal()


```

### Threshold t vs. individual PGS variance

N = 100 000, M = 5000 and t = 0.9, 0.95, 0.99

```{r}
M = 5000 # of SNPs
h2 = 0.5 # heritability
N = 100000 # of individuals
ts = c(0.9, 0.95, 0.99) # PGS quantiles 
certain.proportions = matrix(nrow = length(ts), ncol = 2)
pgs = vector("list", length(Ns))
pgs.vars = vector("list", length(Ns))
ordered = matrix(nrow = length(ts), ncol = 2) # matrix for storing proportions of certain PGS estimates

pgs.unc = PGS.uncertainty(N, M, h2, S)
pgs[[i]] = pgs.unc$pgs # PGSs for N = Ns[i]
pgs.vars[[i]] = pgs.unc$variance # variances of PGSs for N = Ns[i]
  
for (i in 1:length(ts)){
  ord = ordered.estimates(pgs.unc$pgs, pgs.unc$variance, ts[i], rho)
  ordered[i, ] = c(ord$prop.certain.below, ord$prop.certain.above)
}
colnames(ordered) = c("Below t", " Above t")
rownames(ordered) = c("t=0.9", "t=0.95", "t=9.99")
ordered # proportions of certain-below-t and certain-above-t

all = unlist(pgs) # all pgs concatenated
all.var = unlist(pgs.vars) # all pgs variances concatenated
#sample.size = rep(Ns, times = Ns) # matching vector of sample sizes for each PGS and variance value
#df.var = data.frame(N = sample.size, Variance = all.var) # data frame for plotting
# plot variances of individual PGSs as a function of # of individuals, add prop.confident as label for each Ns[i]


```

The proportion of certain-above-threshold PGSs decreases and proportion of certain-below-threshold PGSs increases with higher cutoff quantile t.

```{r}
colnames(ordered) = c("Certain-below", "Certain-above")
rownames(ordered) = c("t = 0.9", "t = 0.95", "t = 0.99")
ordered
kable(ordered)
```





