---
title: "rapsa"
output: html_document
date: "2025-07-22"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

- Ei tarvitse pitkää johdantoa, vaan lyhyt pohjustus aiheeseen. 
- Teksti ja sisältö saa olla melko teknistä, ja kaikkea ei tarvitse selittää yksinkertaisesti (raportti on omaksi ja Matin luettavaksi). 
- Sisällytä olennaiset koodit ja simulaatiot.
- Sisällytä keskeisimmät havainnot, sekä esimerkkejä epävarmuustekijöistä.
- Selosta kaksi tapaa käsitellä ja käyttää PRSiä.
- Näytä esimerkkejä joissa simulaation parametrit vastaavat julkaistuja tutkimukseia, esim CHD epävarmuus
--> toisaalta näytä miten realistisilla parametreilla PRS epävarmuus pienenee

### muistiinpanot

*summary.txt*
Uncertainty in the individual Polygenic Risk Scores

We study the uncertainty in INDIVIDUALs PGSs by looking at both the numerical values of the PGS estimate, and the ranking of the individual made by ordering the PGS estimates of individuals from the same GWAS. 
In research applications, individuals PGS is compared to a reference threshold value for high genetic risk. The threshold value is computed in large subset of a population, and aims to represent the true genetic value at the top-n% risk threshold.

We have looked into the uncertainty in the PGS estimates in multiple ways:
1) Studied the effect of heritability and GWAS training sample size (used to obtain estimates for genetic effects) to the individual PGS variance analytically.

2) Studied the proportion of certain-above -threshold inividuals among those whose PGS point estimate exceeds the threshold, as a function of GWAS sample side and heritability.

We have studied the variability in individuals ranks by:
1) Sampling PGS estimates for individuals, and computing rankings for the individuals in each sample, we have studies the mean ranking of an individual, as well as the range of rankings.

2) For each individual we have studied the proportion of rankings in the top-PGS quantile

Inrelation to the liability threshold model of disease susceptibility, we have:
1) Studied the risk of disease with specific genotype, by sampling environmental effects. 

2) Computed the mean risk of disease by PGS decile, studying the liabilities of the individuals stratified by PGS.

3) Calculated the liability of individuals in the top-PGS quantile, as well as the prevalence of disease among those individuals.

Finally, we 
1) Estimated the true Polygenic Risk distribution in the population and compute the PGS values at percentiles. Looked into the stability of the percentiles as a function of sample size.

2) Estimate PGSs for individuals using genetic effects from "GWAS", and compare the PGS percentiles to the true values as a function of the estimation sample size.

3) Look into the error in estimating the top-PGS-quantile values with different estimation sample sizes.

Kuinka usein yksilö, joka oikeasti 90th kvantiililla, on estimaattien mukaan siellä



### Introduction (short background)
Polygenic risk scores (PRS) are increasingly used to estimate an individual’s genetic disease risk using variants found in GWAS. There are hopes to use PRSs in clinical setting to [kohdentaa] screenings and preventative treatments to those in high risk of disease. The accuracy of PRSs is usually studied with cohort-level metrics such as R^2 and AUC, but the individual-level accuracy of prediction is less studied.

PRSs are mainly usen in two ways: (1) ranking individuals withing a cohort by their PRS and comparing the relative disease risk across quantiles, and (2) estimating individuals absolute disease risk by comparing to the standardized PRSs in a reference population.
Both approaches are sensitive to uncertainty arising from limited GWAS sample size and uncertainty about causal variants.
This report summarizes simulations exploring how such uncertainty propagates to PRS estimates, individual rankings, and disease risk predictions.

### Methods
#### PGS computation
To calculate PRSs for a set of individuals over M SNPs, we first generate allele frequencies for said SNPs by drawing them from uniform distribution. We simulate genetic effects for each variant (betas) by drawing them from a normal distribution with mean zero and variance $ h2/((2*ps*(1-ps))^S*M)$, with S = - 0.75, as in [1]. This way the variance explained by the variants is scaled to match heritability h2, and more common variants have smaller effects. We assume the M SNPs to be independent, mening we do not consider LD.

Assuming the genotypes are standardized to have mean zero and unit variance, we simulate the genetic effects obtained from GWAS by adding random noise around the true genetic effects. Following [2], we draw the genetic effect estimate for each variant from a normal distribution centered at the true genetic effect, with variance $1/N *(1 - h2/((2*ps*(1-ps))^S*M))$.

We generate genotypes for n individuals, and standardize them. As SNPs with no variants contribute identically to the PRS of each individual, we filter out the SNPs with no variation in them. 

Polygenic risk scores are computed for each individual as the sum of risk allele counts, weighted by their genetic effect size. As we have generated both the true genetic effect sizes (betas) and the estimates usually obtained from GWAS, we can compute both the true genetic value of an individual, as well as PRS samples corresponding to different GWASs.

The variance in individual PRS estimates can be calculated either analytically or empirically from the PRS samples. 
```{r}
PGS.uncertainty <- function(N, M, n, h2, S = -0.75, n.samples = 1000){
  # N: # of individuals
  # M: # of SNPs
  # n: # of individuals whose PGS is to be computed
  # h2: heritability
  # S: relative contributions of common vs rare variants
  # n.samples: # of samples where PGS and its variance are computed from
  
  ps = runif(M, 0, 1) #  allele 1 frequencies for M SNPs
  var = h2/((2*ps*(1-ps))^S*M) # variance proportional to negative selection coefficient, scaled to match heritability h2 and M SNPs
  betas = rnorm(M, mean = 0, sd = sqrt(var)) # draw true betas with variance var.neg
  
  beta.samples = matrix(nrow = n.samples, ncol = M) # collect n.samples of beta estimates
  
  sigma.est = 1/N *(1 - var) # effects of SNPs can be estimated with variance proportional to their allele frequency (rare alleles have larger effect sizes, and larger effect sizes can be estimated with smaller uncertainty)
  
  for (i in 1:n.samples){
    beta.samples[i,] = rnorm(M, betas, sqrt(sigma.est)) # add noise proportional to sample size, formula from paper 3
  }
  
  X <- matrix(rbinom(n * M, size = 2, prob = rep(ps, each = n)), nrow = n, ncol = M) # generate genotypes assuming allele frequencies follow HWE
  
  # filter out variants with no variation across individuals
  idx.filter = apply(X, 2, sd) > 0
  X.filtered = X[, idx.filter]
  beta.samples = beta.samples[, idx.filter] 
  sigma.est = sigma.est[idx.filter]
  
  X.scaled = scale(X.filtered) # standardize genotypes to have mean 0 and variance 1
  beta.mean = colMeans(beta.samples) # mean of the sampled beta estimates, acts as the point estimate
  pgs.samples = matrix(nrow = n.samples, ncol = n) # each column is a sample of PGSs for an individual
  
  gv = X.scaled %*% betas[idx.filter] # true genetic values
  
  beta.est = beta.samples[1, ] # single effect estimates for analytical variance calculation
  
  pgs.samples = beta.samples %*% t(X.scaled)
  
  pgs = colMeans(pgs.samples) # point estimates for PGSs
  #pgs.var = X^2 %*% sigma.est # variance in individual PGS estimates due to noise in the effect size estimates
  pgs.var = rowSums((X.filtered^2) * sigma.est) #   PITÄISIKÖ LASKEA SKAALATUILLA GENOTYYPEILLÄ??????
  pgs.sample.var = apply(pgs.samples, 2, var) # variances of individual PGS estimates
  
  return(list(gv = gv, # true genetic values
              pgs = pgs, # PGS point estimates
              pgs.samples = pgs.samples, # a (n.samples x n) matrix of n.samples PGS estimates for n individuals
              sample.variance = pgs.sample.var, # variance in individual PGS estimates due to noise in the effect size estimates, from analytical formula
              variance = pgs.var # variances of individual PGS estimates, estimated from the sample
              ))
}
```

#### Liability threshold model
We perform simulations using the liability threshold model, following [3]. An individuals liability value is the sum of genetic effects and environmental effects. We use a lifetime disease prevalence value for a specific disease to obtain the liability threshold, and those individuals whose liability value exceeds the threshold have the disease. 

To gain a more realistic understanding of the liability threshold model, we compute the Polygenic Scores for the individuals, and use the scaled PGS as the genetic component in the liability value. Here we consider a qualitative phenotype, so we must transform the genetic effects into the liability scale using the disease prevalence.
```{r}
N = 100000
M = 1000
n = 1000
h2 = 0.5
K = 0.1
t = qnorm(1-K) # threshold liability for disease based on prevalence K. People with liability > t have the disease
PGSs = PGS.uncertainty(N, M, n, h2) # Generate genotypes and compute PGS
PGS = PGSs$pgs
PGS.scaled = scale(PGS) # scale PGSs to have mean = 0 and var = 1
e = rnorm(n, 0, sqrt(1-h2)) # environmental effects
g = PGS.scaled * sqrt(h2) # genetic effects

l = g + e # liability values

length(which(l > t))
disease.status = ifelse(l > t, 1, 0) # assign disease status
```

```{r}
cat("Simulated disease prevalence:", mean(disease.status), "\n")

# Plot liability distribution
library(ggplot2)
df = data.frame(Liability = l, Disease = factor(disease.status))

ggplot(df, aes(x = Liability, fill = Disease)) +
  geom_density(alpha = 0.5) +
  geom_vline(xintercept = t) +
  labs(title = "Liability threshold model",
       subtitle = paste("simulated disease prevalence =", round(mean(disease.status),3)),
       fill = "Disease Status") +
  theme_minimal()

 ggplot(df, aes(x = Liability, fill = Disease)) +
  geom_histogram(alpha = 0.5, position = "identity", bins = 50) +
  geom_vline(xintercept = t) +
  labs(title = "Liability threshold model",
       subtitle = paste("simulated disease prevalence =", round(mean(disease.status),3)),
       fill = "Disease Status") +
  theme_minimal()
```


#### Uncertainty in PRS estimates

Uncertainty in individual PRS estimates arises from several sources, the main one being limited GWAS sample size N. This is incorporated into our simulations as the variance of the genetic effect estimates is proportional to N. Furthermore, we have uncertainty on which variants are truly the causal ones, but this is not included in our simulations. In the simulation setting uncertainty can stem from small target sample size n, which makes the ordering of individuals by their PRS highly variable.

In the simulations we explore the uncertainty by varying heritability h2, GWAS samples size N, target population size n and disease prevalence K. 

### Simulations

#### Liability threshold model simulations
We studied the uncertainty in the computed liability value caused by the uncertainty in the PGS bu keeping the environmental effects constant, and varying the genetic effects. It seems that there is major uncertainty in the liability value for about XX % of the individuals, as the proportion of the sampled liability values is in (0.4, 0.6).

We looked into the risk of disease with a specific genotype by keeping the genetic effects constant and sampling the environmental effects for each of the individuals.
We keep the genetic effects constant and sample environmental effects to get a disease risk estimate for each individual.
```{r}
N = 100000 # of individuals
Ks = c(0.1, 0.01, 0.001) # lifetime prevalence of disease
M = 1000 # of SNPs
h2s = c(0.2, 0.5, 0.8) # heritability 
n = 1000

risk.by.K = list()
decile.summary = data.frame()

for (K in Ks) {
  t = qnorm(1 - K)  # liability threshold for this prevalence
  # threshold liability for disease based on prevalence K. People with liability > t have the disease

  risk.by.h2 = list() 

  for (h2 in h2s){
  # sample N different genetic effects
  PGSs = PGS.uncertainty(N, M, n, h2) # Generate genotypes and compute PGS
  PGS = PGSs$pgs
  PGS.scaled = scale(PGS) # scale PGSs to have mean = 0 and var = 1
  g = as.vector(PGS.scaled * sqrt(h2)) # genetic effects
  
  # sample environmental effects: tail probability = risk of disease
  prop.over.t = pnorm(t, g, sqrt(1-h2), lower.tail = FALSE)
  
  # calculate liabilities, n.samples liabilities for each individual
  risk.by.h2[[as.character(h2)]] = prop.over.t # collect proportion of top PGS quantile samples for each individual
  
  cutpoints = quantile(g, probs = seq(0,1,0.01)) 
  deciles = cut(g, breaks = cutpoints, include.lowest = TRUE, labels = FALSE)
    
  for (d in 1:100) {
      idx = which(deciles == d)
      mean.risk = mean(prop.over.t[idx])
      sd.risk = sd(prop.over.t[idx])
      decile.summary = rbind(decile.summary,
                      data.frame(K = K, h2 = h2, decile = d, mean.risk = mean.risk, sd.risk = sd.risk))
}
  }
  risk.by.K[[as.character(K)]] = risk.by.h2
}

```
Plot mean risk of disease by PGS decile by heritability

```{r}
colors = rainbow(length(Ks))
for (K in Ks) {
  subset.K = decile.summary[decile.summary$K == K, ] # decile summary for current K
  plot(1:100, type = "n", ylim = c(0, max(subset.K$mean.risk)), 
       xlab = "PGS decile", ylab = "Mean risk",
       main = paste0("Mean disease risk by PGS decile (K = ", K, ")"))

  for (i in seq_along(h2s)) {
    h2 = h2s[i]
    subset.h2 = subset.K[subset.K$h2 == h2, ]
    lines(subset.h2$decile, subset.h2$mean.risk, type = "b", 
          col = colors[i], lwd = 2, pch = 16)
  }

  legend("topleft", legend = paste0("h2 = ", h2s), 
         col = colors, lwd = 2, pch = 16)
}

```
Finally, we computed the prevalence of disease among the individuals in the top PGS quantile. This was computed as the proportion of individuals in the top PGS quantile, whose liability values exceed the liability threshold.
```{r}

N = 10000 # number of individuals
M = 1000 # number of SNPs
h2 = 0.5 # heritability on liability scale
K = 0.1 # disease prevalence
tPGS = 0.90 # top PGS quantile threshold
S = -1 # selection parameter 

PGSs = PGS.sample.uncertainty.liability(N, M, h2, -1, K, n.samples) # Generate genotypes and compute PGS
PGS = PGSs$pgs 
g = as.vector(scale(PGS) * sqrt(h2)) # scaled genetic effects

e = rnorm(N, 0, sqrt(1 - h2)) # environmental component

l = g + e # total liability

t = qnorm(1 - K) # threshold for disease liability
cases = which(l > t) # who has disease

top.idx = which(N*tPGS < rank(PGS, ties.method = "first")) # individual PGSs in the top PGS quantile

prevalence.top = mean(l[top.idx] > t) # prevalence in top PGS quantile

prevalence.overall = mean(l > t) # overall prevalence

cat("Prevalence :", prevalence.overall, "\n")
cat("Prevalence in top", 100*(1 - tPGS), "% PGS quantile:", prevalence.top, "\n")

```

#### Ranking simulations
In this file we do simulations on the relative rankings of individual Polygenic Scores. As the PGS value itself can vary between GWASes and simulations, we look at the relative rankings, which are in some sense a more objective measure of the relative risk of an individual wrt the rest of the population. We study how variable the relative rankings of individual Polygenic Scores are between simulation rounds, and which proportion of individuals are "certain" above or below the top PGS quantile. We sample genotypes and effects sizes, estimate the effect sizes and compute polygenic scores. We order the PGS estimates and collect their rankings. When the effect sizes are sampled and PGS rankings collected several times, we can observe how much the individual rankings change due to the effect estimation uncertainty. We collect
- PGS rankings for each individual
- Proportion of PGS rankings in the top PGS quantile for each individual (can be interpreted as the probability of being in the top PGS quantile)
- Correlations of two rankings of the same individual 

Looking at the proportion of rankings in the top PGS quantile for each individual, the maximum proportion appropaches 1.0 as heritability and GWAS sample size increases. For sample size > 50 000, we can reliably classify the top PGS individuals even with very low heritability. For small sample size and small heritability, even the individuals with highest PGS estimates have significant uncertainty in them, and are not in the top PGS quantile with high probability. We also observe that the proportion of individual PGSs over threshold t approaches either 1 or 0 with increasing sample size and heritability.


```{r}
# ----------------------------------------------------------------------------------------------------
# Function for calculating sample PGS estimates and their relative rankings, and proportions
# of ranking exceeding the PGS stratification quantile t
# ----------------------------------------------------------------------------------------------------
PGS.sample.rankings <- function(N, M, n, h2, n.samples = 1000, S = -0.75, t = 0.9){
  # N: # of individuals in GWAS used to estimate effect sizes
  # M: # of SNPs
  # n: # of individuals whose PGS is to be computed
  # h2: heritability
  # n.samples: # of PGS estimates to be sampled
  # S: relative contributions of common vs rare variants
  # t: PGS quantile to be considered
  
  ps = runif(M, 0, 1) #  allele 1 frequencies for M SNPs
  var = h2/((2*ps*(1-ps))^S*M) # variance proportional to negative selection coefficient, scaled to match heritability h2 and M SNPs
  sigma.est = 1/N *(1 - var) # effects of SNPs can be estimated with variance proportional to their allele frequency (rare alleles have larger effect sizes, and larger effect sizes can be estimated with smaller uncertainty)
  betas = rnorm(M, mean = 0, sd = sqrt(var)) # draw betas with variance var.neg
  
  X <- matrix(rbinom(n * M, size = 2, prob = rep(ps, each = n)), nrow = n, ncol = M) # generate genotypes for n individuals assuming allele frequencies follow HWE (n x M)
  
  # filter out SNPs with no variation
  idx.filter = apply(X, 2, sd) > 0
  X = X[, idx.filter]
  X = scale(X, center = TRUE, scale = TRUE)
  betas = betas[idx.filter] 
  sigma.est = sigma.est[idx.filter]
  
  M.true = length(betas)
  
  # sample effect estimates
  beta.samples = matrix(nrow = n.samples, ncol = M.true) # collect n.samples of beta estimates (n.samples x M (or less if filtered SNPs))
  for (i in 1:n.samples){ # sample beta estimates and collect into beta.samples matrix
    beta.samples[i,] = rnorm(M.true, betas, sqrt(sigma.est)) # add noise proportional to sample size, formula from paper 3
  }
  
  # compute PGS estimates
  pgs.est = X %*% t(beta.samples) # PGS samples (n x n.samples)
  pgs.mean = rowMeans(pgs.est) # PGS point estimates for each individual
  
  # compute rankings of PGS estimates
  rankings = matrix(nrow = n, ncol = n.samples) # a (n x n.samples) matrix of rankings
  
  for (i in 1:n.samples) {
    ind = 1:n # indices of PGS point estimates
    rankings[, i] = as.integer(rank(pgs.est[, i], ties.method = "first")) # order indices from smallest PGS to largest
      #order(pgs.est[,i], decreasing = FALSE) 
     
  }
  t.ind = ceiling(t*n) # cutoff index
  rank.prop = apply(rankings, 1, function(x){mean(x > t.ind)
    #return(mean(above))
  })
  
  return(list(
    geno = X, # a (n x M) matrix of genotypes of n individuals on M SNPs
    beta = betas, # a vector of the true effects of M SNPs
    p = ps, # a vector of allele frequencies at M SNPs
    pgs = pgs.mean, # a vector of PGS point estimates for n individuals
    pgs.est = pgs.est, # a (n x n.samples) matrix of PGS samples
    pgs.rankings = rankings, # a (n x n.samples) matrix of rankings of n.samples PGS estimates for n individuals
    prop.over.t = rank.prop # a vector of proportions of PGS values over t:th quantile for each individual
    )) 
}
```

For each individual we also compute the mean and median rankings, as well as the range of the rankings, which describes well how variable the individual rankings are. We see that the mean range of the rankings decreases with increasing heritability and GWAS sample size N. However, even with large sample size and heritability, there is large variation in the range of the PGS rankings between individuals, which seems to indicate that some individuals can be more reliably classified as a high- or low-risk individuals than others. 

Collect PGS rankings, correlations and proportions for N = 500 000 and h2 = 0.5
```{r}
M = 1000 # of SNPs
h2 = 0.5 # heritability
N = 500000 # of individuals in GWAS training data
n.samples = 10000 #
t = 0.9 # 
n = 10000 # of individuals whose PGSs are computed
n.stats = 5 # of statistics to be collected for each individual
len.h2 = length(h2s) # of different heritabilities

ranking.stats = matrix(nrow = n, ncol = n.stats ) 
rank.correlations = matrix(nrow = n, ncol = 1) 
proportions = matrix(nrow = n, ncol = 1) # proportion of PGS estimates over t for each individual, for each N and h2

pgs.sample.rankings = PGS.sample.rankings(N, M, n, h2, n.samples, S = -1, t)

# collect stats
rankings = pgs.sample.rankings$pgs.rankings # output is a (n x n.samples) matrix of rankings of n.samples PGS estimates for n individuals
mean = rowMeans(rankings) # mean PGS for each individual
median = apply(rankings, 1, median) # median PGS for each individual
min.ranking = apply(rankings, 1, min) # minimums of individuals' rankings
max.ranking = apply(rankings, 1, max) # maximums of individuals' rankings
range = max.ranking - min.ranking # range of rankings of an individual

ranking.stats = cbind(mean, median, min.ranking, max.ranking, range)

# compute rank correlations
rank.correlations = compute.rank.correlations(rankings)

# collect proportions
pgs.prop = pgs.sample.rankings$prop.over.t
proportions = pgs.prop

colnames(ranking.stats) = c("mean", "median", "min", "max", "range") # set column names

head(ranking.stats)

range.means = mean(ranking.stats[, 5])

```
```{r}
# consider only N = 500000 and h2 = 0.5
rankings.Nlarge = ranking.stats[,c(3, 4, 5)]

# order rows by mean from smallest to largest
ordered.rankings = rankings.Nlarge[order(rankings.Nlarge[, 1]), ]

n = nrow(ordered.rankings) # split the ordered matrix into 10 deciles
decile_size = n / 10

# MEAN
means = ordered.rankings[,1] # extract means 
mean.deciles = split(means, rep(1:10, each = decile_size))

# compute the mean of the means in each decile
mean_of_means = sapply(mean.deciles, function(mat) round(mean(mat)))

# MIN
mins = ordered.rankings[,2] # extract mins
min.deciles = split(mins, rep(1:10, each = decile_size))

# compute the min of the mins in each decile
min_of_mins = sapply(min.deciles, function(mat) min(mat))

# MAX
maxs = ordered.rankings[,3] # extract maxs
max.deciles = split(maxs, rep(1:10, each = decile_size))

# compute the max of the maxs in each decile
max_of_maxs = sapply(max.deciles, function(mat) max(mat))

# RANGE
ranges = max_of_maxs - min_of_mins # max ranges in each decile

# TABLE 
ranking.table = cbind(mean_of_means, ranges, min_of_mins, max_of_maxs)
colnames(ranking.table) = c("Mean rankings", "Range", "Min ranking", "Max ranking")
ranking.table
```
Finally, we visualized the individual PGS estimates as well as their rankings. For individuals that have rankings both above- and below-threshold we notice that some below-threshold PGS values are larger than some above-threshold PGS values, and vice versa. This depends mostly on the target sample size n, so that with smaller n the overlap is larger. This demonstrates that the PGS values themselves are only to be used in ranking individuals with effect estimates from the same GWAS, unless the target sample is large or the PGS values are standardized.
```{r}
# Load required libraries
library(ggplot2)
library(patchwork)

# Parameters
M = 1000
h2 = 0.5
N = 50000
n.samples = 100
t = 0.9
n = 10000

# Simulate data
v.rankings = PGS.sample.rankings(N, M, n, h2, n.samples, S = -1, t)
pgs.rankings = v.rankings$pgs.rankings  # a (n x n.samples) matrix of PGS samples (every row corresponds to an individual)
pgs.est = v.rankings$pgs.est  # a (n x n.samples) matrix of rankings of n.samples PGS estimates for n individuals (every row corresponds to an individual)

i = 30 # Individual index

## Plot rankings
ind.rankings = pgs.rankings[i, ] # take all sample rankings of the ith individual
q.vals1 = rep(t * n, times = n.samples) # a vector of quantile values
col1 = ifelse(ind.rankings > q.vals1, "above t", "below t") # label rankings based on if they are above or below the quantile
df1 = data.frame(x = ind.rankings, y = 0, col = factor(col1, levels = c("below t", "above t")))
pct.above = mean(col1 == "above t") * 100

plot1 = ggplot(df1, aes(x = x, y = y, color = col)) +
  geom_point(position = position_jitter(height = 0.05), size = 2) +
  geom_vline(xintercept = t*n, linetype = "dashed", color = "black") +
  scale_color_manual(values = c("above t" = "red", "below t" = "blue")) +
  theme_minimal() +
  theme(
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank()
  ) +
  labs(
    title = "Distribution of individual PGS rankings, n = 10 000",
    x = "Value",
    color = paste0("> ", t * 100, "th percentile")
  ) +
  coord_cartesian(ylim = c(-0.5, 0.5)) 

## Plot PGS estimates
ind.est = pgs.est[i, ]  # take all sample PGS estimates of the ith individual
q.est = apply(pgs.est, 2, quantile, probs = t) # calculate values at tth quantile at each columns
col2 = ifelse(ind.est > q.est, "above t", "below t") # label PGSs based on if they are above or below the quantile
df2 = data.frame(x = ind.est, y = 0, col = factor(col2, levels = c("below t", "above t")))

plot2 = ggplot(df2, aes(x = x, y = y, color = col)) +
  geom_point(position = position_jitter(height = 0.05), size = 2) +
  scale_color_manual(values = c("above t" = "red", "below t" = "blue")) +
  theme_minimal() +
  theme(
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank()
  ) +
  labs(
    title = "Distribution of individual PGS estimates",
    x = "Value",
    color = paste0("> ", t * 100, "th percentile")
  ) +
  coord_cartesian(ylim = c(-0.5, 0.5)) +
  annotate("text", x = Inf, y = 0.4, label = paste0(round(pct.above, 1), "% above t"), 
           hjust = 1.1, size = 4, color = "black")

combined_plot = plot1 / plot2
combined_plot
```

We also studied the order-correlation of individual's PGS estimates by comparing the rankings from two PGS estimations. The correlations increase significantly with GWAS training sample size, and moderately with increasing heritability, and with our example N = 500 000 and h2 = 0.5, the rankings are very stable.
```{r}
# summarize order correlations
cor.vec <- as.vector(rank.correlations)

summary.stats <- c(
  Mean = mean(cor.vec, na.rm = TRUE),
  Min = min(cor.vec, na.rm = TRUE),
  Max = max(cor.vec, na.rm = TRUE),
  SD   = sd(cor.vec, na.rm = TRUE),
  Median = median(cor.vec, na.rm = TRUE)
)

round(summary.stats, 3)

```

!!!! YKSILÖIDEN RANKING-JAKAUMAT TÄHÄN !!!!
- sample ranking funktio, vai voiko yhdistää aiempaan funktioon, tai ainakin käyttää sen outputtia ja tehdä lyhemmän funktion ?

#### Population risk estimation

We want to model the true polygenic risk in population. To do this we first generate the phenotypes and true genetic effects (betas). Using the true betas, we
compute the polygenic score for a large number of individuals, yielding us the true genetic values of said individuals. We use large enough N to be able to precisely estimate the genetic values at the quantiles of the PGS distribution. 

We test how large N would be needed, so that there is only little variation in the GV percentiles between simulations. We notice that with larger sample size the standard deviation in the sample percentiles remains smaller. An exception is the 0th and the 100th quantile, which are in practice the minimum and maximum value in the PGS data. With 400 000 individuals the PGS percentiles remain relatively stable between simulation.

After this we estimate the genetic effects as if we would observe the effect estimates from a GWAS. Now we consider another sample of individuals (of size n), and estimate their polygenic scores using the estimated betas. We study the uncertainty in the PGS estimates in two ways:
1) The uncertainty caused by the limited size n of the sample of individuals whose PGS is computed. If n is too small, small changes in the PGS value can cause major shifts in the ranking of an individual. The sample size n should represent the size of a realistic sample of individuals whose PGS were to be estimated in a real-life setting. 

To study the uncertainty caused by the limited sample size used to calculate the PGS ranks, we simulate PGS values and compute the values at percentiles for different n. We expect that with larger n, the PGS percentiles would estimate the GV percentiles better than with smaller n. We calculate the absolute difference between the simulated and the "true" percentiles, and observe that with n = 10 000 the absolute difference to the true percentiles remains small and varies less than with smaller n. 
  
2) The uncertainty caused by the uncertainty in the beta estimates obtained from the "GWAS". 

For both of these factors we look at how the PGS values at quantiles are affected by the uncertainty. Is is crucial to consider how the PGS values at the top PGS quantile vary, as it may directly affect clinical decisions. To do this we simulate PGS estimates for different n, and look at which proportion of the top-PGS estimates are truly in the top-GV quantile. We should further study if the PGS estimates over- or underestimate the risk of individuals.

#### Uncertainty propagated to risk estimates
Miten epävarmuus PGS estimoinnissa välittyy sairastumisriskin arvioinnin epävarmuudeksi. 
- Simuloin pgs otokset ja laskin riskit eri prevalensseille (GWAS N = 500 000, PGS n = 10 000, h² = 0.5, K = 0.001, 0.01, 0.1). Jokaiselle yksilölle laskin 1000 PGS-estimaattia ja vertasin näistä laskettuja sairastumisriskejä todelliseen, oikeilla betoilla laskettuun PGS:äön perustuvaan sairastumisriskiin.
Katsoin riskejä tarkemmin kahdelle eri yksilöjoukolle:
- 10 yksilöä, joilla PGS-estimaattien hajonta on suurin, ja
- 10 yksilöä, joilla PGS:n avulla laskettujen sairastumisriskien hajonta on suurin.
Piirsin boxplotit, joissa näkyy yksilöiden sairastumisriskin vaihtelu PGS-estimaattien välillä. Punaisella pisteellä on merkitty kunkin yksilön todellinen sairastumisriski. 

Ensimmäisten kuvaajien yksilöillä, joilla PGS-estimaattien hajonta on suurinta, sairastumisriskin hajonta vaihtelee. Vaikka PGS hajonta olisi suurta, jos he sijoittuvat geneettisen komponentin osalta kauas liabiliteettirajasta, vaihtelu sairastumisriskissä ei ole yhtä suurta.

Toisessa kuvaajassa yksilöillä, joiden sairastumisriski vaihtelee eniten, on kaikilla korkea todellinen sairastumisriski. Nämä yksilöt sijoittuvat geneettisen komponentin osalta jo lähelle liabiliteettirajaa, jolloin pienet muutokset PGS:ssö johtavat suuriin muutoksiin sairastumisriskissä.

Molemmissa ryhmissä yksilöiden sairastumisriskissä on suurta vaihtelua, esim. esimmäisessä kuvassa yksilön 1 riski vaihtelee välillä 12.1 % - 50,4 %., ja toisessa kuvassa yksilön 1 riski on välillä 24,8 % - 66,8 %.
Piirsin myös sairastumisriskin PGS:n funktiona, ja merkitsin siihen nämä kaksi yksilöjoukkoa. Suurimman PGS-hajonnan yksilöt sijoittuvat hajalleen koko käyrälle, mutta suurimman riskihajonnan yksilöt keskittyvät käyrän jyrkimpään kohtaan.

Kuvat näyttävät, millaista vaihtelua yksittäisten yksilöiden kohdalla voi esiintyä niin PGS:ssä kuin sairastumisriskissä. Näissä kuvissa yksilöiden riski lasketaan suoraan PGS estimaatista, joten yksilöiden keskinäinen järjestys simulaatioissa ei vaikuta riskiestimaatteihin. 

Boxplotin korkeus kertoo miten epävarmuus PGS estimoinnissa siirtyy epävarmuuteen sairausriskin estimoinnissa.

1) Valittu 10 yksilöä, joiden PGS varianssi on suurin. 
-> Yksilöt joiden PGS estimaatti on epävarma, mutta saattavat sijoittua kaikkialle PGS jakaumalla. 

2) Valittu 10 yksilöä, joiden PGS avulla laskettujen sairastumisriskien varianssi on suurin. 
-> Yksilöt, joiden sairausriski on vaihtelee paljon eri PGS estimaattien välillä. Näille yksilöille pienet muutokset PGS arvossa aiheuttavat suuria muutoksia sairausriskissä. (sijoittuvat jyrkkään kohtaa liabiliteetti-käyrällä)
-> Kertoo epävarmuudesta sairastumisriskissä. 

 Kuvaajissa laskettu sairastumisriski yksilöille eri PGS estimaateilla, sekä "todellinen sairastumsiriski". Laskut on toistettu eri prevalensseille.

```{r}

library(ggplot2)
library(reshape2)

N = 500000
n = 10000
M = 1000
h2 = 0.5
n.samples = 1000
Ks = c(0.001, 0.01, 0.1) # disease prevalences

pgs = PGS.uncertainty(N, n, M, h2)

pgs.samples = pgs$pgs.samples # a (n.samples x n) matrix of n.samples PGS estimates for n individuals
pgs.gv = pgs$gv # true GVs of n individuals

for (K in Ks){
  t = qnorm(1 - K)  # liability threshold for this prevalence  
  true.risks = pnorm(t, pgs.gv, sqrt(1-h2), lower.tail = FALSE) # true risk of individuals, tail probability of genetic value -centered random environmental effects
  
  # RISK OF DISEASE
  sample.risks = pnorm(t, pgs.samples, sqrt(1 - h2), lower.tail = FALSE) # risk of disease for each PGS sample as a tail probability
  
  # TOP TEN INDICES BY PGS SD
  sds = apply(pgs.samples, 2, sd)
  risk.filter = true.risks > 0.01 
  sd.ordered = order(sds, decreasing = TRUE)
  sd.indices = sd.ordered[risk.filter[sd.ordered]][1:10] # interesting indices by pgs sd
  #print(sd.indices)
  
  # TOP TEN INDICES BY RISK SD
  risk.sd = apply(sample.risks, 2, sd) 
  risk.indices = order(risk.sd, decreasing = TRUE)[1:10] # interesting individuals by risk sd
  #print(risk.indices)
  # Plot risk of disease for individuals with most variation in PGS
  plot.risks = sample.risks[,sd.indices]
  true.risks.i = true.risks[sd.indices]
  
  # PRINT SDs
  risk.sd.i = risk.sd[sd.indices]
  print(paste0("Risk sd of individuals with largest sd in PGS: ", 
             paste(round(risk.sd.i, 5), collapse = ", ")))

  risk.sd.i = risk.sd[risk.indices]
  print(paste0("Risk sd of individuals with largest sd in risk: ", 
             paste(round(risk.sd[risk.indices], 5), collapse = ", ")))
  
  risks.long = melt(plot.risks, varnames = c("Sample", "PGS_Index"), value.name = "Risk")
  risks.long$RiskPct = risks.long$Risk * 100
  
  real.indices = sd.indices # or risk.indices depending on plot
  #risks.long$Real_Index = factor(real.indices[risks.long$PGS_Index])
 
   # Print ranges for individuals with largest PGS variance
  cat("Ranges of disease risk for individuals with largest PGS variance:\n")
  for (i in seq_along(sd.indices)) {
    idx <- sd.indices[i]
    risks_i <- sample.risks[, idx]
    range_i <- range(risks_i)
    cat(sprintf("Individual %d: %.2f%% – %.2f%%\n", i, range_i[1]*100, range_i[2]*100))
  }
  # print boxplots of risks for top pgs sd individuals
  print(ggplot(risks.long, aes(x = factor(PGS_Index), y = RiskPct)) +
    geom_boxplot(fill = "lightblue") +
    geom_point(data = data.frame(PGS_Index = factor(1:ncol(plot.risks)), Risk = true.risks.i * 100), 
             aes(x = PGS_Index, y = Risk), 
             color = "red", size = 2) +
    labs(title = paste0("Risk of Disease for 10 individuals with most variation in PGS estimates, K = ", K), x = "Individual (ordered by PGS SD)", y = "Risk of disease (%)") +
    theme_minimal())
  ggsave(filename = paste0("PGS_variation_K_", K, ".png"), 
       plot = last_plot(), 
       width = 8, height = 5, dpi = 300, bg = "white")
  
  # Plot risk of disease for individuals with most variation in disease risk
  plot.risks = sample.risks[,risk.indices]
  true.risks.i = true.risks[risk.indices]
  
  risks.long = melt(plot.risks, varnames = c("Sample", "PGS_Index"), value.name = "Risk")
  risks.long$RiskPct = risks.long$Risk * 100
  
  real.indices = risk.indices # or risk.indices depending on plot
  #risks.long$Real_Index = factor(real.indices[risks.long$PGS_Index])
  
  # Print ranges for individuals with largest risk variance
  cat("Ranges of disease risk for individuals with largest disease risk variance:\n")
  for (i in seq_along(risk.indices)) {
    idx <- risk.indices[i]
    risks_i <- sample.risks[, idx]
    range_i <- range(risks_i)
    cat(sprintf("Individual %d: %.2f%% – %.2f%%\n", i, range_i[1]*100, range_i[2]*100))
  }
  
  # print boxplots of risks for top pgs sd individuals
  print(ggplot(risks.long, aes(x = factor(PGS_Index), y = RiskPct)) +
    geom_boxplot(fill = "lightblue") +
    geom_point(data = data.frame(PGS_Index = factor(1:ncol(plot.risks)), Risk = true.risks.i * 100), 
             aes(x = PGS_Index, y = Risk), 
             color = "red", size = 2) +
    labs(title = paste0("Risk of Disease for 10 individuals with most variation in disease risk, K = ", K), x = "Individual (ordered by risk SD)", y = "Risk of disease (%)") +
    theme_minimal())
  
  ggsave(filename = paste0("Risk_variation_K_", K, ".png"), 
       plot = last_plot(), 
       width = 8, height = 5, dpi = 300, bg = "white")

  }

```
```{r}
# Riskifunktio (cumulative normal)
pgs.range = seq(min(pgs$gv) - 1, max(pgs$gv) + 1, length.out = 1000)
risk.curve = pnorm(t, mean = pgs.range, sd = sqrt(1 - h2), lower.tail = FALSE)

plot(pgs.range, risk.curve, type = "l", col = "blue",
     xlab = "PGS estimate", ylab = "Disease risk",
     main = "Disease risk vs. PGS")

# Individuals with largest PGS variance
points(pgs$gv[sd.indices], true.risks[sd.indices], col = "red", pch = 19)
#text(pgs$pgs.est[sd.indices], true.risks[sd.indices],
#     labels = sd.indices, pos = 3, col = "red")

#  Individuals with largest variance in disease risk
points(pgs$gv[risk.indices], true.risks[risk.indices], col = "darkgreen", pch = 17)
#text(pgs$pgs.est[risk.indices], true.risks[risk.indices],
#     labels = risk.indices, pos = 1, col = "darkgreen")

legend("bottomright", legend = c("Top PGS variance", "Top disease risk variance"),
       col = c("red", "darkgreen"), pch = c(19,17))

```




Kun n = 1000 tai 10 000, epävarmuus pienenee kun K kasvaa. Kun yksilöjoukko on pieni (n = 100), niin epävarmuus on suurinta kun K = 0.01 (vs 0.1 tai 0.001). 
- kun n = 100, prevalenssilla 0.1 sairastuneita yksilöitä on noin 10, prevalenssilla 0.01 sairastuneita on noin 1 ja prevalenssilla 0.001 sairastuneita ei todennäköisesti ole. Siten prevalensseilla 0.1 ja 0.001 simulaatioissa näkyy melko pientä epävarmuutta sairastumisriskissä, mutta prevalenssilla 0.01 sairastuneita on joko 0 tai 1, mikä aiheuttaa korkeamman epävarmuuden riskiin.

Muuten epävarmuus riskissä pienenee kun prevalenssi kasvaa, sillä kun sairastuneita on enemmän, pienet vaihtelut sairastuneiden lukumääärissä eivät aiheuta suurta epävarmuutta sairastumisriskiin.

Tapausten lukumäärän kasvaessa epävramuus sairastumisriskissä pienenee. Kun tapausten määrä kasvaa, efektiivinen otoskoko kasvaa ja epävarmuus GWAS betoissa pienenee. Tällöin epävarmuus yksilöiden PGS estimaateissa pienenee ja myös epävarmuus sairastumsiriskissä pienenee. 

Heritabiliteetin h2 kasvaessa riskikäyrä jyrkkenee, kun ympäristön vaikutus sairastumisriksiin pienenee. Simulaatioissa heritabiliteetin ja sairastumisriskin epävarmuuden välillä ei näy selkeää yhteyttä. 

Yksilöjoukon koko n, jossa PGS lasketaan ja yksilöt järjestetään, vaikuttaa sairastumisriskin epävarmuuteen merkittävästi; epävarmuus on suurempaa kun n on pieni. Erityisesti kun n = 100, satunnaisuus vaikuttaa suuresti. 
```{r}
# ---------------------------------------------------------------------------------------------------------------------
# a function for computing disease risks (min, max and mean risk, range and variance) for pgs ranking positions over a sample of individuals 
# ---------------------------------------------------------------------------------------------------------------------
# ???? VOISIKO YHDISTÄÄ AIEMPAAN FUNKTIOON, TAI KÄYTTÄÄ SEN OUTPUTTIA JA TEHDÄ TÄSTÄ LYHEMMÄN ????
PGS.risk.by.ranking <- function(N, M, n, h2, K, n.samples, S = -1){
  # N: # of individuals in GWAS used to estimate effect sizes
  # M: # of SNPs
  # n: # of individuals whose PGS is to be computed
  # h2: heritability
  # n.samples: # of PGS estimates to be sampled
  # S: relative contributions of common vs rare variants
  # q: PGS quantile to be considered
  
  t = qnorm(1 - K)
  
  ps = runif(M, 0, 1) #  allele 1 frequencies for M SNPs
  var = h2/((2*ps*(1-ps))^S*M) # variance proportional to negative selection coefficient, scaled to match heritability h2 and M SNPs
  betas = rnorm(M, mean = 0, sd = sqrt(var)) # draw betas with variance var.neg
  
  X <- matrix(rbinom(n * M, size = 2, prob = rep(ps, each = n)), nrow = n, ncol = M) # generate genotypes for n individuals assuming allele frequencies follow HWE (n x M)
  
  # filter out SNPs with no variation
  idx.filter = apply(X, 2, sd) > 0
  X = X[, idx.filter]
  X = scale(X, center = TRUE, scale = TRUE)
  betas = betas[idx.filter] 
  
  M.true = length(betas)
  
  sigma.est = 1/N *(1 - var) # effects of SNPs can be estimated with variance proportional to their allele frequency (rare alleles have larger effect sizes, and larger effect sizes can be estimated with smaller uncertainty)
  sigma.est = sigma.est[idx.filter]
  
  # sample effect estimates
  beta.samples = matrix(nrow = n.samples, ncol = M.true) # collect n.samples of beta estimates (n.samples x M (or less if filtered SNPs))
  for (i in 1:n.samples){ # sample beta estimates and collect into beta.samples matrix
    beta.samples[i,] = rnorm(M.true, betas, sqrt(sigma.est)) # add noise proportional to sample size, formula from paper 3
  }
  
  # compute PGS estimates
  pgs.est = X %*% t(beta.samples) # PGS samples (n x n.samples)
  
  # scale pgs estimates
  pgs.sc = scale(pgs.est)
  
  # order PGS estimates within each sample (order each column of pgs.est)
  pgs.ord = apply(pgs.sc, 2, sort) # now each column (sample) is sorted into increasing order
  
  # compute risk for each pgs estimate
  risks = pnorm(t, pgs.ord, sqrt(1-h2), lower.tail = FALSE) # a (n x n.samples) matrix of risks
  print(N)
  print(dim(risks))
  # compute minimum, maximum and mean risks for each n (point in the PGS distribution)
  min.risks = apply(risks, 1, min)
  max.risks = apply(risks, 1, max)
  mean.risks = apply(risks, 1, mean)
   
  pgs.mean = colMeans(pgs.est) # PGS point estimates for each individual
  
  # compute rankings of PGS estimates
  rankings = matrix(nrow = n, ncol = n.samples) # a (n x n.samples) matrix of rankings

  
  return(list(
    pgs = pgs.mean, # a vector of PGS point estimates for n individuals
    pgs.est = pgs.est, # a (n x n.samples) matrix of PGS samples
    risks = risks,
    min.risks = min.risks, 
    max.risks = max.risks, 
    mean.risks = mean.risks
    )) 
}

```

```{r}
N = 100000
M = 1000
n = 1000 # vaihtele
h2 = 0.2 # vaihtele
K = 0.01 # vaihtele
n.samples = 1000 # vai 100

PGS.risk = PGS.risk.by.ranking(N, M, n, h2, K, n.samples)

risks = PGS.risk$risks # (n x n.samples) matrix of risks 
max(PGS.risk$max.risks[1:9900] - PGS.risk$min.risks[1:9900])

t = qnorm(1 - K)
pgs_thresh_percentile <- 1 - pnorm(t / sqrt(h2))
pgs_thresh_rank <- pgs_thresh_percentile # already scaled

cat(sprintf("PGS threshold at %.2f%% percentile for prevalence K=%.2f\n",
            pgs_thresh_rank * 100, K * 100))

# plot risks by PGS ranking
library(ggplot2)

# df for min/max risks
min_max_df <- data.frame(
  Rank = (1:n) / n,  # Scale rank to [0,1]
  Min = PGS.risk$min.risks,
  Max = PGS.risk$max.risks
)
max_allowed_rank <- 0.999
max_allowed_idx <- floor(n * max_allowed_rank)

uncertainty <- PGS.risk$max.risks - PGS.risk$min.risks
max_idx <- which.max(uncertainty[1:max_allowed_idx])       # index of max uncertainty
max_rank <- max_idx / n                 # percentile (scaled rank)
max_diff <- uncertainty[max_idx]        # max difference (risk range)
max_min <- PGS.risk$min.risks[max_idx]  # min risk at this percentile
max_max <- PGS.risk$max.risks[max_idx]  # max risk at this percentile

PGS.sorted <- sort(PGS.risk$PGS)  # sorted PGS scores
PGS.threshold <- PGS.sorted[max_idx]  # PGS value at max uncertainty

# tulosta tiivistelmä
cat(sprintf(
  "Max uncertainty at percentile %.1f%%: %.2f%% – %.2f%% (Δ=%.2f%%)\n",
  max_rank * 100, max_min * 100, max_max * 100, max_diff * 100
))
cat(sprintf("PGS threshold at this percentile: %.4f\n", PGS.threshold))

ggplot(min_max_df, aes(x = Rank)) +
  geom_ribbon(aes(ymin = Min, ymax = Max), fill = "lightblue", alpha = 0.4) +
  geom_line(aes(y = Min), color = "red", size = 1) +
  geom_line(aes(y = Max), color = "blue", size = 1) +
  geom_vline(xintercept = max_rank, linetype = "dashed", color = "black") + # vertical line
  annotate("point", x = max_rank, y = PGS.risk$min.risks[max_idx], color = "red", size = 2) +
  annotate("point", x = max_rank, y = PGS.risk$max.risks[max_idx], color = "blue", size = 2) +
  
  geom_vline(xintercept = pgs_thresh_rank, linetype = "dotted", color = "purple") + # prevalence threshold
  labs(title = "Min and Max risks across PGS rankings",
       subtitle = sprintf("Max uncertainty at %.1f%% percentile with risk between %.2f%% and %.2f%%", round(max_rank * 100,1), round(max_min*100,1), round(max_max*100, 1)),
       x = "Relative PGS rank",
       y = "Risk") +
  theme_minimal()

```

```{r}
library(ggplot2)
library(grid)

# a function for plotting min and max risks over pgs percentiles

plot_PGS_uncertainty <- function(N.case = 100000, N.ctrl = 100000, M = 1000, n = 1000, h2 = 0.2, K = 0.01, n.samples = 100) {
  
  # compute effective sample size
  phi = N.case/(N.ctrl + N.case) # phi: proportion of cases
  N.eff = (N.case + N.ctrl) * phi * (1-phi)
  
  # Run the risk computation
  PGS.risk <- PGS.risk.by.ranking(N.eff, M, n, h2, K, n.samples)
  
  # Prevalence liability threshold
  t <- qnorm(1 - K)
  pgs_thresh_percentile <- pnorm(t / sqrt(h2))
  pgs_thresh_rank <- pgs_thresh_percentile
  
  min_max_df <- data.frame(
    Rank = (1:n) / n,
    Min = PGS.risk$min.risks,
    Max = PGS.risk$max.risks
  )
  
  max_allowed_rank <- 0.999
  max_allowed_idx <- floor(n * max_allowed_rank)
  
  uncertainty <- PGS.risk$max.risks - PGS.risk$min.risks
  max_idx <- which.max(uncertainty[1:max_allowed_idx])
  max_rank <- max_idx / n
  max_diff <- uncertainty[max_idx]
  max_min <- PGS.risk$min.risks[max_idx]
  max_max <- PGS.risk$max.risks[max_idx]
  
  PGS.sorted <- sort(PGS.risk$PGS)
  PGS.threshold <- PGS.sorted[max_idx]
  
  cat(sprintf(
    "Max uncertainty at percentile %.1d%%: %.2f%% – %.2f%% (Δ=%.2f%%)\n",
   round(max_rank * 100), max_min * 100, max_max * 100, max_diff * 100
  ))
  cat(sprintf("PGS threshold at this percentile: %.4f\n", PGS.threshold))
  cat(sprintf("PGS threshold percentile for prevalence (K=%.3f): %.2f%%\n", K, pgs_thresh_rank * 100))
  
  param_text <- paste(
    sprintf("N.cases = %d", round(N.case)),
    sprintf("N.ctrl = %d", round(N.ctrl)),
    sprintf("M = %d", M),
    sprintf("n = %d", n),
    sprintf("h2 = %.2f", h2),
    sprintf("K = %.3f", K),
    sep = "\n"
  )
  
  # tekstiboksin muotoilu
  param_grob <- grobTree(
    rectGrob(gp = gpar(fill = "white", col = "black", alpha = 0.8)),
    textGrob(param_text, x = 0.02, y = 0.98, just = c("left", "top"),
             gp = gpar(col = "black", fontsize = 10, fontface = "bold"))
  )
  
  # Laske ymin ja ymax, vähän yli riskien ylärajan, tekstiboksin rajat
  
  ymax_pos <- max(min_max_df$Max) * 1.05
  ymin_pos <- ymax_pos - 0.37
  
  p <- ggplot(min_max_df, aes(x = Rank)) +
    geom_ribbon(aes(ymin = Min, ymax = Max), fill = "lightblue", alpha = 0.4) +
    geom_line(aes(y = Min), color = "red", size = 1) +
    geom_line(aes(y = Max), color = "blue", size = 1) +
    annotate("point", x = max_rank, y = max_min, color = "red", size = 3) +
    annotate("point", x = max_rank, y = max_max, color = "blue", size = 3) +
    labs(
      title = "Min and Max risks across PGS rankings",
      subtitle = sprintf(
        "Max uncertainty at %.1dth percentile with risk %.2f%%–%.2f%% ",
        round(max_rank * 100), max_min * 100, max_max * 100
      ),
      x = "Relative PGS rank",
      y = "Risk"
    ) +
    theme_minimal() +
    annotation_custom(param_grob, xmin = 0, xmax = 0.2, ymin = ymin_pos, ymax = ymax_pos)
  
  print(p)
  
  invisible(list(
    max_uncertainty_percentile = max_rank,
    max_uncertainty_delta = max_diff,
    pgs_threshold_percentile = pgs_thresh_rank,
    pgs_threshold_value = PGS.threshold
  ))
}


```

### Example cases
```{r}
#CHD PGS000019 : h2 = 0.5, K = 0.05, N.case = 60 000, N.ctrl = 120 000, M = 192, n = 725, samples
h2 = 0.5 
K = 0.05 
N.case = 60000 
N.ctrl = 120000 
M = 192
n = 725
samples = 1000
plot_PGS_uncertainty(N.case, N.ctrl, M, n, h2, K, samples)

```

#### Simulations matching CHD parameters

Verification of liability distribution.
PGS scaling, thresholding, and uncertainty propagation.
Uncertainty in disease risk with fixed genotype vs. varying environment.
3.2 PGS ranking simulations
Variability in individual rankings between simulations.
Proportion of individuals reliably above/below a threshold.
Correlation between rankings as a function of N and h².
3.3 Population risk estimation
Stability of PGS percentiles with increasing sample size.
Uncertainty from finite estimation sample and GWAS beta estimation.
3.4 Uncertainty propagation to disease risk estimates
Boxplots for individuals with highest PGS variance and risk variance.
Effect of prevalence, sample size, and heritability.
Example Cases (realistic parameters)
Simulations matching CHD parameters (e.g. h² ≈ 0.4, N ≈ 500k).
Show how uncertainty in PGS/risk diminishes with large N.
Discussion
Summary of key findings on uncertainty.
Implications for using PRS in research and clinical settings.
Appendix
Essential code snippets and additional figures.



Lähetä matille:
ranking distributionit mutta sairastumisriskillä / riskiplotit todellisten tutkimusten parametreillä
pitääkö betat skaalata?


[1] Theoretical and empirical quantification of the accuracy of polygenic scores in ancestry divergent populations
[2] Large uncertainty in individual polygenic risk score estimation impacts PRS-based risk stratification
[3] Heritable polygenic editing: the next frontier in genomic medicine?

