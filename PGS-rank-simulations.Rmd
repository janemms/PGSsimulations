---
title: "PGS simulations"
output: html_document
date: "2025-06-19"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
In this file we do simulations on the relative rankings of individual Polygenic Scores. As the PGS value itself can vary between GWASes and simulations, we look at the relative rankings, which are in some sense a more objective measure of the relative risk of an individual wrt the rest of the population. We study how variable the relative rankings of individual Polygenic Scores are between simulation rounds, and which proportion of individuals are "certain" above or below the top PGS quantile. We sample genotypes and effects sizes, estimate the effect sizes and compute polygenic scores. We order the PGS estimates and collect their rankings. When the effect sizes are sampled and PGS rankings collected several times, we can observe how much the individual rankings change due to the effect estimation uncertainty. We collect
- PGS rankings for each individual
- Proportion of PGS rankings in the top PGS quantile for each individual (can be interpreted as the probability of being in the top PGS quantile)
- Correlations of two rankings of the same individual 

For each individual we also compute the mean and median rankings, as well as the range of the rankings, which describes well how variable the individual rankings are. We see that the mean range of the rankings decreases with increasing heritability and GWAS sample size N. However, even with large sample size and heritability, there is large variation in the range of the PGS rankings between individuals, which seems to indicate that some individuals can be more reliably classified as a high- or low-risk individuals than others. 

Looking at the proportion of rankings in the top PGS quantile for each individual, the maximum proportion appropaches 1.0 as heritability and GWAS sample size increases. For sample size > 50 000, we can reliably classify the top PGS individuals even with very low heritability. For small sample size and small heritability, even the individuals with highest PGS estimates have significant uncertainty in them, and are not in the top PGS quantile with high probability. We also observe that the proportion of individual PGSs over threshold t approaches either 1 or 0 with increasing sample size and heritability.

We also studied the order-correlation of individual's PGS estimates by comparing the rankings from two PGS estimations. From the violin-plots we see that the correlation increases significantly with GWAS training sample size, and moderately with increasing heritability.

Finally, we visualized the individual PGS estimates as well as their rankings. For individuals that have rankings both above- and below-threshold we notice that some below-threshold PGS values are larger than some above-threshold PGS values, and vice versa. This demonstrates that the PGS values themselves are only to be used in ranking individuals with effect estimates from the same GWAS.

## Sampling PGS relative rankings

Collect PGS rankings, correlations and proportions in one loop.
```{r}
source("PGSvariance.R")
M = 1000 # of SNPs
h2s = c(0.2, 0.5, 0.8) # heritabilities
Ns = c(1000, 10000, 100000, 250000, 500000) # of individuals in GWAS training data
n.samples = 10000 #
t = 0.9 # 
n = 10000 # of indivuduals whose PGSs are computed
n.stats = 5 # of statistics to be collected for each individual
len.h2 = length(h2s) # of different heritabilities

ranking.stats = matrix(nrow = n, ncol = length(h2s) * n.stats * length(Ns)) 
rank.correlations = matrix(nrow = n, ncol = length(h2s)*length(Ns)) 
proportions = matrix(nrow = n, ncol = length(h2s)*length(Ns)) # proportion of PGS estimates over t for each individual, for each N and h2

for (i in 1:length(Ns)){
  for (j in 1:length(h2s)){
    pgs.sample.rankings = PGS.sample.rankings(Ns[i], M, n, h2s[j], n.samples, S = -1, t)
    
    # collect stats
    rankings = pgs.sample.rankings$pgs.rankings
    # output is a (n x n.samples) matrix of rankings of n.samples PGS estimates for n individuals
    mean = rowMeans(rankings) # mean PGS for each individual
    median = apply(rankings, 1, median) # median PGS for each individual
    min.ranking = apply(rankings, 1, min) # minimums of individuals' rankings
    max.ranking = apply(rankings, 1, max) # maximums of individuals' rankings
    range = max.ranking - min.ranking # range of rankings of an individual
    col.start = ((i - 1) * len.h2 + (j - 1)) * n.stats + 1
    col.end = col.start + n.stats - 1

    ranking.stats[, col.start:col.end] = c(mean, median, min.ranking, max.ranking, range)
    
    # compute rank correlations
    rank.correlations[,((i-1)*length(h2s)+j)] = compute.rank.correlations(rankings)
    
    # collect proportions
    pgs.prop = pgs.sample.rankings$prop.over.t
    proportions[,((i-1)*length(h2s)+j)] = pgs.prop
  }
  
}

```


### Range of rankings and mean/median rankings

```{r}
stat.names = c("mean", "median", "min", "max", "range")
col.names = c()

for (i in seq_along(Ns)) {
  for (j in seq_along(h2s)) {
    for (stat in stat.names) {
      col.names = c(col.names, paste0("N", Ns[i], ".h2.", h2s[j], ".", stat))
    }
  }
}
colnames(ranking.stats) = col.names # set column names

head(ranking.stats)

```

```{r}
# mean range of rankings for each N and h2
range.cols = seq(from = n.stats, to = ncol(ranking.stats), by = n.stats)
ranges = ranking.stats[,range.cols]

range.means = colMeans(ranges)

range.mat = matrix(round(range.means), nrow = length(Ns), ncol = length(h2s), byrow = FALSE) # format

rownames(range.mat) = paste0("N = ", Ns)
colnames(range.mat) = paste0("h2 = ", h2s)

range.mat
```


```{r}
# consider only N = 500000 and h2 = 0.5
rankings.Nlarge = ranking.stats[,c(66, 68, 69)]

# order rows by mean from smallest to largest
ordered.rankings = rankings.Nlarge[order(rankings.Nlarge[, 1]), ]

n = nrow(ordered.rankings) # split the ordered matrix into 10 deciles
decile_size = n / 10

# MEAN
means = ordered.rankings[,1] # extract means 
mean.deciles = split(means, rep(1:10, each = decile_size))

# compute the mean of the means in each decile
mean_of_means = sapply(mean.deciles, function(mat) round(mean(mat)))

# MIN
mins = ordered.rankings[,2] # extract mins
min.deciles = split(mins, rep(1:10, each = decile_size))

# compute the mean of the means in each decile
min_of_mins = sapply(min.deciles, function(mat) min(mat))

# MAX
maxs = ordered.rankings[,3] # extract maxs
max.deciles = split(maxs, rep(1:10, each = decile_size))

# compute the mean of the means in each decile
max_of_maxs = sapply(max.deciles, function(mat) max(mat))

# RANGE
ranges = max_of_maxs - min_of_mins # max ranges in each decile

# TABLE 
ranking.table = cbind(mean_of_means, ranges, min_of_mins, max_of_maxs)
colnames(ranking.table) = c("Mean rankings", "Range", "Min ranking", "Max ranking")
ranking.table
```

### Proportion of individual PGS estimates over t

```{r}
col.names = c()

for (i in seq_along(Ns)) {
  for (j in seq_along(h2s)) {
      col.names = c(col.names, paste0("N", Ns[i], ".h2.", h2s[j]))
  }
}
colnames(proportions) = col.names # set column names

```

```{r}
apply(proportions, 2, max)
```

```{r}
# Split into groups for clearer plotting
n.groups = 510  # number of groups 

group.means = matrix(NA, nrow = n.groups, ncol = ncol(proportions)) # a matrix to store means: rows = groups, cols = N-h2-groups

for (col.idx in 1:(ncol(proportions))) { # iterate over columns (N-h2-groups)
  col.values = proportions[, col.idx] 
  
  ordered.indices = order(col.values, decreasing = TRUE) # order indices by decreasing value
  
  group.sizes = rep(floor(length(col.values)/n.groups), n.groups) # split indices into n.groups equal groups
  remainder = length(col.values) %% n.groups # check if jako ei mennyt tasan
  if (remainder > 0) { 
    group.sizes[1:remainder] = group.sizes[1:remainder] + 1 # add one to remainder first groups
  }
  
  groups = rep(1:n.groups, times = group.sizes) # assign groups to indices
  grouped.values = split(col.values[ordered.indices], groups) # get the values as a list of groups
  group.means[, col.idx] = sapply(grouped.values, mean, na.rm = TRUE) # compute means for each group
}

rownames(group.means) = paste0("Group", 1:n.groups)
colnames(group.means) = colnames(proportions)


```

```{r}
library(reshape2)
library(ggplot2)

n.individuals = nrow(proportions) # dimensions (n)

N.rep = rep(Ns, each = length(h2s)) # metadata for the columns
h2.rep = rep(h2s, times = length(Ns))

prop.df = as.data.frame(group.means) # convert to data frame
colnames(prop.df) = paste0("N", N.rep, ".h2.", h2.rep)

prop.long = reshape2::melt(prop.df, variable.name = "Condition", value.name = "Proportion") # reshape to long format

prop.long$N = as.numeric(sub("^N([0-9.e+-]+)\\.h2\\..*$", "\\1", prop.long$Condition))
prop.long$h2 = as.numeric(sub(".*\\.h2\\.(.*)", "\\1", prop.long$Condition))

prop.long
```

```{r}
ggplot(prop.long, aes(x = as.numeric(factor(N), -0.3, 0.3), 
                      y = Proportion, 
                      color = factor(h2))) +
  geom_point(alpha = 0.6, size = 2, position = position_jitter(width = 0.05), size = 2) +
  scale_x_continuous(breaks = 1:length(Ns), labels = Ns) +
  labs(x = "Sample size (N)", y = "Proportion of rankings over t", color = "hÂ²") +
  theme_minimal()


```
Proportion of individual PGSs over threshold t approaches either 1 or 0 with increasing sample size and heritability.


Separate plots for heritabilities
```{r}
split_by_h2 <- split(prop.long, prop.long$h2)

# Plot each separately
plot_list <- lapply(names(split_by_h2), function(h2_val) {
  ggplot(split_by_h2[[h2_val]], aes(x = as.numeric(factor(N)), 
                                    y = Proportion)) +
    geom_point(alpha = 0.6, size = 2, color = "steelblue",  position = position_jitter(width = 0.05), size = 2) +
    scale_x_continuous(breaks = 1:length(Ns), labels = Ns) +
    labs(title = paste("Proportion of Rankings over t for hÂ² =", h2_val),
         x = "Sample size (N)", 
         y = "Proportion") +
    theme_minimal()
})

# Display plots
for (p in plot_list) {
  print(p)
}
```


## Order-correlation of sampled PGS estimates

```{r}
col.names = c()
N.names = c()
h2.names = c()

for (i in seq_along(Ns)) {
  for (j in seq_along(h2s)) {
      col.names = c(col.names, paste0("N", Ns[i], ".h2.", h2s[j]))
      N.names = c(N.names, Ns[i])
      h2.names = c(h2.names, h2s[j])
  }
}
colnames(rank.correlations) = col.names # set column names
head(rank.correlations)
colMeans(rank.correlations)

```

```{r}
library(knitr)
# violin plot rank correlations
rank.df = na.omit(as.data.frame(rank.correlations)) # to df
#rank.df$N = N.names
#rank.df$h2 = h2.names
rank.long = reshape2::melt(rank.df, variable.name = "Condition", value.name = "Correlation") # reshape to long format


rank.long$N = as.numeric(sub("^N([0-9.e+-]+)\\.h2\\..*$", "\\1", rank.long$Condition))
rank.long$h2 = as.numeric(sub(".*\\.h2\\.(.*)", "\\1", rank.long$Condition))

mean.matrix = with(rank.long, tapply(Correlation, list(N, h2), mean))
colnames(mean.matrix) = paste0("h2 = ", colnames(mean.matrix))

Ns_ordered = sort(unique(rank.long$N)) # order rows by N
mean.matrix = mean.matrix[as.character(Ns_ordered), , drop = FALSE]

kable(round(mean.matrix,3))

plot.df = rank.long[rank.long$h2 == h2s[1], ]

plot.df$N = factor(plot.df$N)       # treat N as categorical for x-axis


for (i in 1:length(h2s)){
  plot.df = rank.long[rank.long$h2 == h2s[i], ]
  pl = ggplot(na.omit(plot.df), aes(factor(N), Correlation))
  print(pl + geom_violin(scale="count") +
  labs(x = "Sample size (N)", fill = "hÂ²") +
  ylim(-0.05,1) +
  theme_minimal())

}


```

# Visualization of individual PGS estimates

```{r}
# Load required libraries
library(ggplot2)
library(patchwork)

source("PGSvariance.R")

# Parameters
M = 1000
h2 = 0.5
N = 50000
n.samples = 100
t = 0.9
n = 10000

# Simulate data
v.rankings = PGS.sample.rankings(N, M, n, h2, n.samples, S = -1, t)
pgs.rankings = v.rankings$pgs.rankings  # a (n x n.samples) matrix of PGS samples (every row corresponds to an individual)
pgs.est = v.rankings$pgs.est  # a (n x n.samples) matrix of rankings of n.samples PGS estimates for n individuals (every row corresponds to an individual)

i = 30 # Individual index

## Plot rankings
ind.rankings = pgs.rankings[i, ] # take all sample rankings of the ith individual
q.vals1 = rep(t * n, times = n.samples) # a vector of quantile values
col1 = ifelse(ind.rankings > q.vals1, "above t", "below t") # label rankings based on if they are above or below the quantile
df1 = data.frame(x = ind.rankings, y = 0, col = factor(col1, levels = c("below t", "above t")))
pct.above = mean(col1 == "above t") * 100

plot1 = ggplot(df1, aes(x = x, y = y, color = col)) +
  geom_point(position = position_jitter(height = 0.05), size = 2) +
  geom_vline(xintercept = t*n, linetype = "dashed", color = "black") +
  scale_color_manual(values = c("above t" = "red", "below t" = "blue")) +
  theme_minimal() +
  theme(
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank()
  ) +
  labs(
    title = "Distribution of individual PGS rankings, n = 10 000",
    x = "Value",
    color = paste0("> ", t * 100, "th percentile")
  ) +
  coord_cartesian(ylim = c(-0.5, 0.5)) 

## Plot PGS estimates

# Correct extraction
ind.est = pgs.est[i, ]  # take all sample PGS estimates of the ith individual
q.est = apply(pgs.est, 2, quantile, probs = t) # calculate values at tth quantile at each columns
col2 = ifelse(ind.est > q.est, "above t", "below t") # label PGSs based on if they are above or below the quantile
df2 = data.frame(x = ind.est, y = 0, col = factor(col2, levels = c("below t", "above t")))

plot2 = ggplot(df2, aes(x = x, y = y, color = col)) +
  geom_point(position = position_jitter(height = 0.05), size = 2) +
  scale_color_manual(values = c("above t" = "red", "below t" = "blue")) +
  theme_minimal() +
  theme(
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank()
  ) +
  labs(
    title = "Distribution of individual PGS estimates",
    x = "Value",
    color = paste0("> ", t * 100, "th percentile")
  ) +
  coord_cartesian(ylim = c(-0.5, 0.5)) +
  annotate("text", x = Inf, y = 0.4, label = paste0(round(pct.above, 1), "% above t"), 
           hjust = 1.1, size = 4, color = "black")

combined_plot = plot1 / plot2
combined_plot
```



#### Top quantile PGS values for each sample
```{r}
M = 1000
h2 = 0.5
N = 50000
n.samples = 1000
t = 0.9
n = 10000

samples = PGS.sample.rankings(N, M, n, h2, n.samples, S = -1, t)$pgs.est 
q.est = apply(samples, 2, quantile, probs = t) # calculate values at tth quantile at each columns
plot(q.est) # variability in top PGS quantile value between samples

```



## Change in rankings, correlation and proportion over t for different N within individuals

Generate genotype data and true genetic effects
```{r}
source("PGSvariance.R")
M = 1000 # of SNPs
n = 10000 
h2 = 0.5
geno = generate.geno(M, n, h2, S = -1) # generate genotypes, betas and collect variance parameters
var = geno$var
beta = geno$beta
X = geno$X

```

```{r}
Ns = c(1000, 10000, 100000, 250000, 500000) # of individuals in GWAS training data
n.samples = 1000 # for each individual

props = matrix(nrow = 4, ncol = length(Ns)) # rows individuals, columns Ns
ranks = matrix(nrow = 4*length(Ns), ncol = n.samples)

for (i in 1:length(Ns)){
  rankings = PGS.sample.rankings.2(N, n.samples, X, var, beta, t = 0.9) 
  props[, i] = rankings$prop.over.t[1:4]  
  ranks[((i-1)*4 + 1): (i*4), ] = rankings$pgs.rankings[1:4, ]
  
  # compute.rank.correlations = function(pgs.ranks, n.pairs = 1000, seed = 42) {
}
```

```{r}
library(patchwork)
# individuals 1, 2, 3 and 4
plots = list()
for (i in 1:4){ # individuals
  for (j in 1:length(Ns)){
    ind.rankings = ranks[(j-1)*4 + i, ]
    
    q.vals = rep(t * n, times = n.samples) #apply(pgs.rankings, 1, quantile, probs = t) # values at tth quantiles
    col = ifelse(ind.rankings > q.vals, "above t", "below t")
    
    df = data.frame(x = ind.rankings, y = 0, col = factor(col, levels = c("below t", "above t")))
    
    print(ggplot(df, aes(x = x, y = y, color = col)) +
      geom_point(position = position_jitter(height = 0.05), size = 2) +
      geom_vline(xintercept = t*n, linetype = "dashed", color = "black") +
      scale_color_manual(values = c("above t" = "red", "below t" = "blue")) +
      theme_minimal() +
      theme(
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank()
      ) +
      labs(
        title = paste0("Individual PGS rankings with N = ", Ns[j]),
        x = "Value",
        color = paste0("> ", t * 100, "th percentile")
      ) +
      coord_cartesian(ylim = c(-0.5, 0.5)))
    
  }
  
}

```

```{r}
props

```


TO DO:
- plot proportion of ambiguous individuals as a function of heritability and GWAS N
- identify areas in pgs distribution where small changes in pgs value lead to large changes is disease risk


```{r}
source("PGSvariance.R")
M = 1000 # of SNPs
h2s = c(0.2, 0.5, 0.8) # heritabilities
N = 500000 # of individuals in GWAS training data
n.samples = 10000 #
t = 0.9 # 
n = 10000 # of indivuduals whose PGSs are computed
n.stats = 6 # of statistics to be collected for each individual
len.h2 = length(h2s) # of different heritabilities

ranking.stats = matrix(nrow = n, ncol = length(h2s) * n.stats) 
rank.correlations = matrix(nrow = n, ncol = length(h2s) )
proportions = matrix(nrow = n, ncol = length(h2s)) # proportion of PGS estimates over t for each individual, for each h2

for (j in 1:length(h2s)){
    pgs.sample.rankings = PGS.sample.rankings(N, M, n, h2s[j], n.samples, S = -1, t)
    
    # collect stats
    rankings = pgs.sample.rankings$pgs.rankings
    # output is a (n x n.samples) matrix of rankings of n.samples PGS estimates for n individuals
    mean = rowMeans(rankings) # mean PGS for each individual
    median = apply(rankings, 1, median) # median PGS for each individual
    min.ranking = apply(rankings, 1, min) # minimums of individuals' rankings
    max.ranking = apply(rankings, 1, max) # maximums of individuals' rankings
    range = max.ranking - min.ranking # range of rankings of an individual
    variance = apply(rankings, 1, var) # variance of rankings of an individual
    col.start = ((j - 1) * n.stats + 1)
    col.end = col.start + n.stats - 1

    ranking.stats[, col.start:col.end] = c(mean, median, min.ranking, max.ranking, range, variance)
    
    # collect proportions
    pgs.prop = pgs.sample.rankings$prop.over.t
    proportions[,j] = pgs.prop
  }
  
```
#### Proportion of ranking over threshold as a function of mean rank or true PGS percentile
```{r}
#proportions
#ranking.stats[, c(1, 6, 11)]

cols <- c("blue", "red", "green")
plot(NULL, xlim = c(0,1), ylim = c(0,1),
     xlab = "True PGS percentile",
     ylab = paste0("Proportion above threshold t=", t),
     main = "Proportion over threshold vs PGS percentile")

for (j in 1:length(h2s)) {
  mean_ranks <- ranking.stats[, (j-1)*n.stats + 1]
  prop_over_t <- proportions[,j]
  ord <- order(mean_ranks)
  lines(mean_ranks[ord]/n.samples, prop_over_t[ord],
        col = cols[j], lwd = 2)
}

legend("topleft", legend=paste("hÂ² =", h2s), col=cols, lwd=2)
abline(v=t, col="black", lty=2)

```
The plot shows the uncertainty in classification for each PGS percentiles. At the tails of the PGS distribution, the individuals have high certainty: they are nearly always ranked the same and their proportion over t is either 0 or 1. 
Near the threshold t there is high uncertainty; these individuals get classified sometimes above and sometimes below t.

Higher heritability decreases the uncertainty. 

#### Variance of individuals rankings, plot variance against mean rank to see which individuals are most ambiguous
```{r}
cols <- c("blue", "red", "green")
plot(NULL, xlim = c(0,1), ylim = c(0,7000),
     xlab = "True PGS percentile",
     ylab = "Range of rankings",
     main = "Range of rankings vs PGS percentile")

for (j in 1:length(h2s)) {
  mean_ranks <- ranking.stats[, (j-1)*n.stats + 1]
  mean_ranks_percentile <- mean_ranks / n
  ranking_range <- ranking.stats[, (j-1)*n.stats + 5]
  ord <- order(mean_ranks_percentile)
  lines(mean_ranks_percentile[ord], ranking_range[ord],
        col = cols[j], lwd = 2)
}

legend("topleft", legend=paste("hÂ² =", h2s), col=cols, lwd=2)
abline(v=t, col="black", lty=2)
```



